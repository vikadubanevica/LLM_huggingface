{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbn-krcblJn5"
      },
      "source": [
        "# Fast tokenizers in the QA pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht1RkL2MlJn-"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHEZoU9_lJn_",
        "outputId": "fbdaf4d2-f880-4fc5-b820-4516d61f4f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmwPOqOjlJoA",
        "outputId": "258d9e81-8e6e-441f-945b-25757ca1b5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.980422704393277,\n",
              " 'start': 78,\n",
              " 'end': 106,\n",
              " 'answer': 'Jax, PyTorch, and TensorFlow'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "context = \"\"\"\n",
        "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
        "question_answerer(question=question, context=context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVB0kqrHlJoC",
        "outputId": "ae9affc8-97cf-4e4b-a9ac-8d81db73c7e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9717117229956784,\n",
              " 'start': 1892,\n",
              " 'end': 1919,\n",
              " 'answer': 'Jax, PyTorch and TensorFlow'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "long_context = \"\"\"\n",
        "ü§ó Transformers: State of the Art NLP\n",
        "\n",
        "ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
        "question answering, summarization, translation, text generation and more in over 100 languages.\n",
        "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
        "\n",
        "ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
        "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
        "can be modified to enable quick research experiments.\n",
        "\n",
        "Why should I use transformers?\n",
        "\n",
        "1. Easy-to-use state-of-the-art models:\n",
        "  - High performance on NLU and NLG tasks.\n",
        "  - Low barrier to entry for educators and practitioners.\n",
        "  - Few user-facing abstractions with just three classes to learn.\n",
        "  - A unified API for using all our pretrained models.\n",
        "  - Lower compute costs, smaller carbon footprint:\n",
        "\n",
        "2. Researchers can share trained models instead of always retraining.\n",
        "  - Practitioners can reduce compute time and production costs.\n",
        "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
        "\n",
        "3. Choose the right framework for every part of a model's lifetime:\n",
        "  - Train state-of-the-art models in 3 lines of code.\n",
        "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
        "  - Seamlessly pick the right framework for training, evaluation and production.\n",
        "\n",
        "4. Easily customize a model or an example to your needs:\n",
        "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
        "  - Model internals are exposed as consistently as possible.\n",
        "  - Model files can be used independently of the library for quick experiments.\n",
        "\n",
        "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question_answerer(question=question, context=long_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-SrRiuWclJoD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI5j6wfflJoE",
        "outputId": "32893381-b6ce-45de-8cc6-7c65040824c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 67]) torch.Size([1, 67])\n"
          ]
        }
      ],
      "source": [
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "print(start_logits.shape, end_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_q30Esh0lJoE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "sequence_ids = inputs.sequence_ids()\n",
        "# Mask everything apart from the tokens of the context\n",
        "mask = [i != 1 for i in sequence_ids]\n",
        "# Unmask the [CLS] token\n",
        "mask[0] = False\n",
        "mask = torch.tensor(mask)[None]\n",
        "\n",
        "start_logits[mask] = -10000\n",
        "end_logits[mask] = -10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TddV7DvalJoF"
      },
      "outputs": [],
      "source": [
        "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
        "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JZFpVH2OlJoF"
      },
      "outputs": [],
      "source": [
        "scores = start_probabilities[:, None] * end_probabilities[None, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "a5Hc-HhblJoG"
      },
      "outputs": [],
      "source": [
        "scores = torch.triu(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5DvU409TiTY",
        "outputId": "b26b3ddb-0816-41b4-ba6a-7278339bd873"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.4340e-13, 0.0000e+00, 0.0000e+00,  ..., 1.1023e-12, 1.6345e-12,\n",
              "         0.0000e+00],\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00],\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00],\n",
              "        ...,\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.1136e-14, 1.3514e-13,\n",
              "         0.0000e+00],\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2744e-13,\n",
              "         0.0000e+00],\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00]], grad_fn=<TriuBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ryZpTQFlJoG",
        "outputId": "77452d2a-fbe4-418a-c8f9-109979425ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "max_index = scores.argmax().item()\n",
        "start_index = max_index // scores.shape[1]\n",
        "end_index = max_index % scores.shape[1]\n",
        "print(scores[start_index, end_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the start and end indices for the five most likely answers."
      ],
      "metadata": {
        "id": "BzNAiziTSfby"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5a9f8e9",
        "outputId": "7076205e-94cd-45f8-8907-636dae59ea52"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Flattening the scores tensor and getting the top 5 indices\n",
        "flat_scores = scores.flatten()\n",
        "top_k_indices = torch.topk(flat_scores, k=5).indices.tolist()\n",
        "\n",
        "results = []\n",
        "for max_index in top_k_indices:\n",
        "    start_index = max_index // scores.shape[1]\n",
        "    end_index = max_index % scores.shape[1]\n",
        "\n",
        "    # Get character offsets from the tokenizer's output\n",
        "    if 'inputs_with_offsets' not in locals():\n",
        "        inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
        "    offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "    # Ensure indices are within bounds for offsets\n",
        "    if start_index < len(offsets) and end_index < len(offsets):\n",
        "        start_char, _ = offsets[start_index]\n",
        "        _, end_char = offsets[end_index]\n",
        "\n",
        "        answer = context[start_char:end_char]\n",
        "        score = scores[start_index, end_index].item()\n",
        "\n",
        "        results.append({\n",
        "            \"answer\": answer,\n",
        "            \"start\": start_char,\n",
        "            \"end\": end_char,\n",
        "            \"score\": score,\n",
        "            \"start_token\": start_index,\n",
        "            \"end_token\": end_index\n",
        "        })\n",
        "\n",
        "# Sorting results by score (highest first)\n",
        "results.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "print(\"Top 5 most likely answers:\")\n",
        "for i, res in enumerate(results):\n",
        "    print(f\"Answer {i+1}:\")\n",
        "    print(f\"  Answer: {res['answer']}\")\n",
        "    print(f\"  Score: {res['score']:.4f}\")\n",
        "    print(f\"  Start Char: {res['start']}, End Char: {res['end']}\")\n",
        "    print(f\"  Start Token: {res['start_token']}, End Token: {res['end_token']}\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most likely answers:\n",
            "Answer 1:\n",
            "  Answer: Jax, PyTorch, and TensorFlow\n",
            "  Score: 0.9803\n",
            "  Start Char: 78, End Char: 106\n",
            "  Start Token: 23, End Token: 35\n",
            "\n",
            "\n",
            "Answer 2:\n",
            "  Answer: Jax, PyTorch, and TensorFlow ‚Äî\n",
            "  Score: 0.0082\n",
            "  Start Char: 78, End Char: 108\n",
            "  Start Token: 23, End Token: 36\n",
            "\n",
            "\n",
            "Answer 3:\n",
            "  Answer: three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow\n",
            "  Score: 0.0068\n",
            "  Start Char: 33, End Char: 106\n",
            "  Start Token: 16, End Token: 35\n",
            "\n",
            "\n",
            "Answer 4:\n",
            "  Answer: Jax, PyTorch\n",
            "  Score: 0.0014\n",
            "  Start Char: 78, End Char: 90\n",
            "  Start Token: 23, End Token: 29\n",
            "\n",
            "\n",
            "Answer 5:\n",
            "  Answer: PyTorch, and TensorFlow\n",
            "  Score: 0.0004\n",
            "  Start Char: 83, End Char: 106\n",
            "  Start Token: 25, End Token: 35\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZySfWd_ZlJoG"
      },
      "outputs": [],
      "source": [
        "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "start_char, _ = offsets[start_index]\n",
        "_, end_char = offsets[end_index]\n",
        "answer = context[start_char:end_char]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVvb1KdGlJoG",
        "outputId": "83353f3d-be23-406e-e5b0-8d65241f1038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'PyTorch, and TensorFlow', 'start': 83, 'end': 106, 'score': tensor(0.0004, grad_fn=<SelectBackward0>)}\n"
          ]
        }
      ],
      "source": [
        "result = {\n",
        "    \"answer\": answer,\n",
        "    \"start\": start_char,\n",
        "    \"end\": end_char,\n",
        "    \"score\": scores[start_index, end_index],\n",
        "}\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass in top_k=5 when calling it."
      ],
      "metadata": {
        "id": "8QgHAXnmSna1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer(question=question, context=long_context, top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c36UjQ85Sopk",
        "outputId": "b615af90-faee-4f97-d554-c381f052068c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9717117229956784,\n",
              "  'start': 1892,\n",
              "  'end': 1919,\n",
              "  'answer': 'Jax, PyTorch and TensorFlow'},\n",
              " {'score': 0.14975238405168056,\n",
              "  'start': 17,\n",
              "  'end': 37,\n",
              "  'answer': 'State of the Art NLP'},\n",
              " {'score': 0.015565173700451851,\n",
              "  'start': 1892,\n",
              "  'end': 1921,\n",
              "  'answer': 'Jax, PyTorch and TensorFlow ‚Äî'},\n",
              " {'score': 0.01435886265244335, 'start': 34, 'end': 37, 'answer': 'NLP'},\n",
              " {'score': 0.010596777312457561,\n",
              "  'start': 3,\n",
              "  'end': 37,\n",
              "  'answer': 'Transformers: State of the Art NLP'}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Handling long contexts\n"
      ],
      "metadata": {
        "id": "e2GPauy9ePMG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmEPSS2NlJoH",
        "outputId": "a5236824-a1cf-4e6b-a25d-98c164b3e0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "461\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(question, long_context)\n",
        "print(len(inputs[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDdqMsarlJoH",
        "outputId": "40cdd7e7-aac6-4f82-8099-b23b80cdab34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model ' s lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
        "print(tokenizer.decode(inputs[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRXfVKoolJoH",
        "outputId": "05023b81-0f3f-407f-a9ab-ebeac6495b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] This sentence is not [SEP]\n",
            "[CLS] is not too long [SEP]\n",
            "[CLS] too long but we [SEP]\n",
            "[CLS] but we are going [SEP]\n",
            "[CLS] are going to split [SEP]\n",
            "[CLS] to split it anyway [SEP]\n",
            "[CLS] it anyway. [SEP]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
        "inputs = tokenizer(\n",
        "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
        ")\n",
        "\n",
        "for ids in inputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-68GHQ7mlJoH",
        "outputId": "388fea02-9bbc-4173-b756-075bb8162a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KeysView({'input_ids': [[101, 1188, 5650, 1110, 1136, 102], [101, 1110, 1136, 1315, 1263, 102], [101, 1315, 1263, 1133, 1195, 102], [101, 1133, 1195, 1132, 1280, 102], [101, 1132, 1280, 1106, 3325, 102], [101, 1106, 3325, 1122, 4050, 102], [101, 1122, 4050, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0]})\n"
          ]
        }
      ],
      "source": [
        "print(inputs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwwpBGtVlJoI",
        "outputId": "e68dfbe9-606f-40ed-987e-7295afa3cb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(inputs[\"overflow_to_sample_mapping\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7yF-avClJoI",
        "outputId": "207d5b7a-74bf-4abe-eb96-83c616f349d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"This sentence is not too long but we are going to split it anyway.\",\n",
        "    \"This sentence is shorter but will still get split.\",\n",
        "]\n",
        "inputs = tokenizer(\n",
        "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
        ")\n",
        "\n",
        "print(inputs[\"overflow_to_sample_mapping\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RjTx-3cDlJoI"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    long_context,\n",
        "    stride=128,\n",
        "    max_length=384,\n",
        "    padding=\"longest\",\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTMPT6lLlJoI",
        "outputId": "e9b8eb4e-6206-4b1b-b44a-be700c481944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 384])\n"
          ]
        }
      ],
      "source": [
        "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "offsets = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "inputs = inputs.convert_to_tensors(\"pt\")\n",
        "print(inputs[\"input_ids\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuAR-y2glJoI",
        "outputId": "748f9dba-1eae-4631-dfaa-d0d745a51ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 384]) torch.Size([2, 384])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "print(start_logits.shape, end_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Xnb4BZanlJoJ"
      },
      "outputs": [],
      "source": [
        "sequence_ids = inputs.sequence_ids()\n",
        "# Mask everything apart from the tokens of the context\n",
        "mask = [i != 1 for i in sequence_ids]\n",
        "# Unmask the [CLS] token\n",
        "mask[0] = False\n",
        "# Mask all the [PAD] tokens\n",
        "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
        "\n",
        "start_logits[mask] = -10000\n",
        "end_logits[mask] = -10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "eXlpMHq7lJoJ"
      },
      "outputs": [],
      "source": [
        "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
        "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmMw5pxwlJoJ",
        "outputId": "66c196ac-4954-48f7-98e1-906e7c6a4a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 18, 0.33866992592811584), (173, 184, 0.9714868664741516)]\n"
          ]
        }
      ],
      "source": [
        "candidates = []\n",
        "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
        "    scores = start_probs[:, None] * end_probs[None, :]\n",
        "    idx = torch.triu(scores).argmax().item()\n",
        "\n",
        "    start_idx = idx // scores.shape[1]\n",
        "    end_idx = idx % scores.shape[1]\n",
        "    score = scores[start_idx, end_idx].item()\n",
        "    candidates.append((start_idx, end_idx, score))\n",
        "\n",
        "print(candidates)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk).\n"
      ],
      "metadata": {
        "id": "D_v36ZUJSx70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = []\n",
        "\n",
        "for batch_idx, (start_probs, end_probs) in enumerate(zip(start_probabilities, end_probabilities)):\n",
        "    # Compute span scores\n",
        "    scores = start_probs[:, None] * end_probs[None, :]\n",
        "\n",
        "    # Keep only valid spans (end >= start)\n",
        "    scores = torch.triu(scores)\n",
        "\n",
        "    # Flatten scores\n",
        "    flat_scores = scores.view(-1)\n",
        "\n",
        "    # Get top-k spans from this example\n",
        "    topk = torch.topk(flat_scores, k=5)\n",
        "\n",
        "    for score, idx in zip(topk.values, topk.indices):\n",
        "        start_idx = idx // scores.shape[1]\n",
        "        end_idx = idx % scores.shape[1]\n",
        "        candidates.append((batch_idx, start_idx, end_idx, score.item()))\n",
        "\n",
        "# Now get the global top-5 across all batches\n",
        "candidates = sorted(candidates, key=lambda x: x[3], reverse=True)[:5]\n",
        "\n",
        "print(candidates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyrgovo1dqdX",
        "outputId": "b04a65be-9032-43c2-9219-38f062aa6c22"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, tensor(173), tensor(184), 0.9714868664741516), (0, tensor(0), tensor(18), 0.33866992592811584), (0, tensor(0), tensor(0), 0.22401432693004608), (0, tensor(13), tensor(18), 0.14949451386928558), (1, tensor(173), tensor(185), 0.015565164387226105)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYTaZTSNlJoJ",
        "outputId": "c8e11c4b-9bf3-4a45-aa38-21127f59a598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': '\\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33866992592811584}\n",
            "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714868664741516}\n"
          ]
        }
      ],
      "source": [
        "for candidate, offset in zip(candidates, offsets):\n",
        "    start_token, end_token, score = candidate\n",
        "    start_char, _ = offset[start_token]\n",
        "    _, end_char = offset[end_token]\n",
        "    answer = long_context[start_char:end_char]\n",
        "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in top_k=5 when calling it."
      ],
      "metadata": {
        "id": "tbx-Bdf-S29B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, start_token, end_token, score in candidates:\n",
        "    offset = offsets[batch_idx]\n",
        "\n",
        "    start_char, _ = offset[start_token]\n",
        "    _, end_char = offset[end_token]\n",
        "\n",
        "    answer = long_context[start_char:end_char]\n",
        "    result = {\n",
        "        \"answer\": answer,\n",
        "        \"start\": start_char,\n",
        "        \"end\": end_char,\n",
        "        \"score\": score,\n",
        "    }\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9i8uYVKS3xg",
        "outputId": "a8166296-eec3-4868-838d-642928ff1014"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714868664741516}\n",
            "{'answer': '\\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33866992592811584}\n",
            "{'answer': '', 'start': 0, 'end': 0, 'score': 0.22401432693004608}\n",
            "{'answer': 'State of the Art NLP', 'start': 17, 'end': 37, 'score': 0.14949451386928558}\n",
            "{'answer': 'Jax, PyTorch and TensorFlow ‚Äî', 'start': 1892, 'end': 1921, 'score': 0.015565164387226105}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer(question=question, context=long_context, top_k=5)"
      ],
      "metadata": {
        "id": "MchBS1SoiB8l",
        "outputId": "b07db895-788a-464b-858f-6466dd2521ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9717117229956784,\n",
              "  'start': 1892,\n",
              "  'end': 1919,\n",
              "  'answer': 'Jax, PyTorch and TensorFlow'},\n",
              " {'score': 0.14975238405168056,\n",
              "  'start': 17,\n",
              "  'end': 37,\n",
              "  'answer': 'State of the Art NLP'},\n",
              " {'score': 0.015565173700451851,\n",
              "  'start': 1892,\n",
              "  'end': 1921,\n",
              "  'answer': 'Jax, PyTorch and TensorFlow ‚Äî'},\n",
              " {'score': 0.01435886265244335, 'start': 34, 'end': 37, 'answer': 'NLP'},\n",
              " {'score': 0.010596777312457561,\n",
              "  'start': 3,\n",
              "  'end': 37,\n",
              "  'answer': 'Transformers: State of the Art NLP'}]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fast tokenizers in the QA pipeline (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}