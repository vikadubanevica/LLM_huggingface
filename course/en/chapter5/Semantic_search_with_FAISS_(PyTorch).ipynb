{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xra2Dpx-lhYb"
      },
      "source": [
        "# Semantic search with FAISS (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEeAgvrAlhYh"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQU4lzzslhYi",
        "outputId": "345f3236-7fd3-40c7-d6c0-88318b8ae43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fViAsVV5lhYj",
        "outputId": "8dc89146-186b-4f27-a6e2-2c73bd86fd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 3019\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73H9fmJtlhYl",
        "outputId": "d2e920c2-293b-4874-c7a0-898a54d831bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 808\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "issues_dataset = issues_dataset.filter(\n",
        "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
        ")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hAx3eJClhYl",
        "outputId": "6e079eea-56f5-4c5d-ee79-fdaa8f198afa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 808\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "columns = issues_dataset.column_names\n",
        "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
        "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_XHxh5aelhYm"
      },
      "outputs": [],
      "source": [
        "issues_dataset.set_format(\"pandas\")\n",
        "df = issues_dataset[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lSrpOeLlhYm",
        "outputId": "a6b8c19c-84d3-4ce8-941d-53cdcfbeaa01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cool, I think we can do both :)',\n",
              " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df[\"comments\"][0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "Spw4i-NOlhYn",
        "outputId": "4ec8d4a4-8f9b-47bb-b8de-1708640665ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            html_url  \\\n",
              "0  https://github.com/huggingface/datasets/issues...   \n",
              "1  https://github.com/huggingface/datasets/issues...   \n",
              "2  https://github.com/huggingface/datasets/issues...   \n",
              "3  https://github.com/huggingface/datasets/issues...   \n",
              "\n",
              "                                               title  \\\n",
              "0                              Protect master branch   \n",
              "1                              Protect master branch   \n",
              "2  Backwards compatibility broken for cached data...   \n",
              "3  Backwards compatibility broken for cached data...   \n",
              "\n",
              "                                            comments  \\\n",
              "0                    Cool, I think we can do both :)   \n",
              "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
              "2  Hi ! I guess the caching mechanism should have...   \n",
              "3  If it's easy enough to implement, then yes ple...   \n",
              "\n",
              "                                                body  \n",
              "0  After accidental merge commit (91c55355b634d0d...  \n",
              "1  After accidental merge commit (91c55355b634d0d...  \n",
              "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
              "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2926cfaa-ce25-401a-bd04-01c7fda35be8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>html_url</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Protect master branch</td>\n",
              "      <td>Cool, I think we can do both :)</td>\n",
              "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Protect master branch</td>\n",
              "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
              "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Backwards compatibility broken for cached data...</td>\n",
              "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
              "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Backwards compatibility broken for cached data...</td>\n",
              "      <td>If it's easy enough to implement, then yes ple...</td>\n",
              "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2926cfaa-ce25-401a-bd04-01c7fda35be8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2926cfaa-ce25-401a-bd04-01c7fda35be8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2926cfaa-ce25-401a-bd04-01c7fda35be8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comments_df",
              "summary": "{\n  \"name\": \"comments_df\",\n  \"rows\": 2964,\n  \"fields\": [\n    {\n      \"column\": \"html_url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 781,\n        \"samples\": [\n          \"https://github.com/huggingface/datasets/issues/615\",\n          \"https://github.com/huggingface/datasets/issues/629\",\n          \"https://github.com/huggingface/datasets/issues/752\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 780,\n        \"samples\": [\n          \"ArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648\",\n          \"dtype of tensors should be preserved\",\n          \"Error loading ms_marco v2.1 using load_dataset()\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2882,\n        \"samples\": [\n          \"cc @beurkinger but I think this has been fixed internally and will soon be updated right ?\",\n          \"Sorry, this is a duplicate of #1287. Not sure why it didn't come up when I searched `iwslt` in the issues list.\",\n          \"You can update the json file by calling\\r\\n```\\r\\nnlp-cli test ./datasets/xtreme --save_infos --all_configs\\r\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 778,\n        \"samples\": [\n          \"Hi, I'm trying to load a dataset from Dataframe, but I get the error:\\r\\n```bash\\r\\n---------------------------------------------------------------------------\\r\\nArrowCapacityError                        Traceback (most recent call last)\\r\\n<ipython-input-7-146b6b495963> in <module>\\r\\n----> 1 dataset = Dataset.from_pandas(emb)\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/site-packages/nlp/arrow_dataset.py in from_pandas(cls, df, features, info, split)\\r\\n    223         info.features = features\\r\\n    224         pa_table: pa.Table = pa.Table.from_pandas(\\r\\n--> 225             df=df, schema=pa.schema(features.type) if features is not None else None\\r\\n    226         )\\r\\n    227         return cls(pa_table, info=info, split=split)\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pandas()\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/pandas_compat.py in dataframe_to_arrays(df, schema, preserve_index, nthreads, columns, safe)\\r\\n    591         for i, maybe_fut in enumerate(arrays):\\r\\n    592             if isinstance(maybe_fut, futures.Future):\\r\\n--> 593                 arrays[i] = maybe_fut.result()\\r\\n    594 \\r\\n    595     types = [x.type for x in arrays]\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/_base.py in result(self, timeout)\\r\\n    426                 raise CancelledError()\\r\\n    427             elif self._state == FINISHED:\\r\\n--> 428                 return self.__get_result()\\r\\n    429 \\r\\n    430             self._condition.wait(timeout)\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/_base.py in __get_result(self)\\r\\n    382     def __get_result(self):\\r\\n    383         if self._exception:\\r\\n--> 384             raise self._exception\\r\\n    385         else:\\r\\n    386             return self._result\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/thread.py in run(self)\\r\\n     55 \\r\\n     56         try:\\r\\n---> 57             result = self.fn(*self.args, **self.kwargs)\\r\\n     58         except BaseException as exc:\\r\\n     59             self.future.set_exception(exc)\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/pandas_compat.py in convert_column(col, field)\\r\\n    557 \\r\\n    558         try:\\r\\n--> 559             result = pa.array(col, type=type_, from_pandas=True, safe=safe)\\r\\n    560         except (pa.ArrowInvalid,\\r\\n    561                 pa.ArrowNotImplementedError,\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()\\r\\n\\r\\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\\r\\n\\r\\nArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648\\r\\n```\\r\\nMy code is :\\r\\n```python\\r\\nfrom nlp import Dataset\\r\\ndataset = Dataset.from_pandas(emb)\\r\\n```\",\n          \"After switching to `datasets` my model just broke. After a weekend of debugging, the issue was that my model could not handle the double that the Dataset provided, as it expected a float (but didn't give a warning, which seems a [PyTorch issue](https://discuss.pytorch.org/t/is-it-required-that-input-and-hidden-for-gru-have-the-same-dtype-float32/96221)). \\r\\n\\r\\nAs a user I did not expect this bug. I have a `map` function that I call on the Dataset that looks like this:\\r\\n\\r\\n```python\\r\\ndef preprocess(sentences: List[str]):\\r\\n    token_ids = [[vocab.to_index(t) for t in s.split()] for s in sentences]\\r\\n\\r\\n    sembeddings = stransformer.encode(sentences)\\r\\n    print(sembeddings.dtype)\\r\\n    return {\\\"input_ids\\\": token_ids, \\\"sembedding\\\": sembeddings}\\r\\n```\\r\\n\\r\\nGiven a list of `sentences` (`List[str]`), it converts those into token_ids on the one hand (list of lists of ints; `List[List[int]]`) and into sentence embeddings on the other (Tensor of dtype `torch.float32`). That means that I actually set the column \\\"sembedding\\\" to a tensor that I as a user expect to be a float32.\\r\\n\\r\\nIt appears though that behind the scenes, this tensor is converted into a **list**. I did not find this documented anywhere but I might have missed it. From a user's perspective this is incredibly important though, because it means you cannot do any data_type or tensor casting yourself in a mapping function! Furthermore, this can lead to issues, as was my case. \\r\\n\\r\\nMy model expected float32 precision, which I thought `sembedding` was because that is what `stransformer.encode` outputs. But behind the scenes this tensor is first cast to a list, and when we then set its format, as below, this column is cast not to float32 but to double precision float64.\\r\\n\\r\\n```python\\r\\ndataset.set_format(type=\\\"torch\\\", columns=[\\\"input_ids\\\", \\\"sembedding\\\"])\\r\\n```\\r\\n\\r\\nThis happens because apparently there is an intermediate step of casting to a **numpy** array (?) **whose dtype creation/deduction is different from torch dtypes** (see the snippet below).  As you can see, this means that the dtype is not preserved: if I got it right, the dataset goes from torch.float32 -> list -> float64 (numpy) -> torch.float64. \\r\\n\\r\\n```python\\r\\nimport torch\\r\\nimport numpy as np\\r\\n\\r\\nl = [-0.03010837361216545, -0.035979013890028, -0.016949838027358055]\\r\\ntorch_tensor = torch.tensor(l)\\r\\nnp_array = np.array(l)\\r\\nnp_to_torch = torch.from_numpy(np_array)\\r\\n\\r\\nprint(torch_tensor.dtype)\\r\\n# torch.float32\\r\\nprint(np_array.dtype)\\r\\n# float64\\r\\nprint(np_to_torch.dtype)\\r\\n# torch.float64\\r\\n```\\r\\n\\r\\nThis might lead to unwanted behaviour. I understand that the whole library is probably built around casting from numpy to other frameworks, so this might be difficult to solve. Perhaps `set_format` should include a `dtypes` option where for each input column the user can specify the wanted precision.\\r\\n\\r\\nThe alternative is that the user needs to cast manually after loading data from the dataset but that does not seem user-friendly, makes the dataset less portable, and might use more space in memory as well as on disk than is actually needed.\",\n          \"Code:\\r\\n`dataset = load_dataset('ms_marco', 'v2.1')`\\r\\n\\r\\nError:\\r\\n```\\r\\n`---------------------------------------------------------------------------\\r\\nJSONDecodeError                           Traceback (most recent call last)\\r\\n<ipython-input-16-34378c057212> in <module>()\\r\\n      9 \\r\\n     10 # Downloading and loading a dataset\\r\\n---> 11 dataset = load_dataset('ms_marco', 'v2.1')\\r\\n\\r\\n10 frames\\r\\n/usr/lib/python3.6/json/decoder.py in raw_decode(self, s, idx)\\r\\n    353         \\\"\\\"\\\"\\r\\n    354         try:\\r\\n--> 355             obj, end = self.scan_once(s, idx)\\r\\n    356         except StopIteration as err:\\r\\n    357             raise JSONDecodeError(\\\"Expecting value\\\", s, err.value) from None\\r\\n\\r\\nJSONDecodeError: Unterminated string starting at: line 1 column 388988661 (char 388988660)\\r\\n`\\r\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "comments_df = df.explode(\"comments\", ignore_index=True)\n",
        "comments_df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFQbZgWTlhYn",
        "outputId": "63aea276-1f44-4d8c-df3a-be4ea16ef60d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 2964\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "comments_dataset = Dataset.from_pandas(comments_df)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z_-5hqQlhYo"
      },
      "outputs": [],
      "source": [
        "comments_dataset = comments_dataset.map(\n",
        "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jreip42elhYo"
      },
      "outputs": [],
      "source": [
        "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uofbWkybremK",
        "outputId": "d10016a1-e5f7-40c6-f41b-51a12e3dc1c8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INRr0cTdlhYo"
      },
      "outputs": [],
      "source": [
        "def concatenate_text(examples):\n",
        "    return {\n",
        "        \"text\": examples[\"title\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"body\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"comments\"]\n",
        "    }\n",
        "\n",
        "\n",
        "comments_dataset = comments_dataset.map(concatenate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5GR_mbMjlhYp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dNulubYlhYp",
        "outputId": "efc3d057-f782-4bb3-de5d-aa527641b12f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MPNetModel(\n",
              "  (embeddings): MPNetEmbeddings(\n",
              "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MPNetEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x MPNetLayer(\n",
              "        (attention): MPNetAttention(\n",
              "          (attn): MPNetSelfAttention(\n",
              "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (intermediate): MPNetIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): MPNetOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (relative_attention_bias): Embedding(32, 12)\n",
              "  )\n",
              "  (pooler): MPNetPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YQKAAI9zlhYp"
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0xFHWdDrlhYp"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(text_list):\n",
        "    encoded_input = tokenizer(\n",
        "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "    model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzZPBj3KlhYp",
        "outputId": "4ff96c6b-8651-49af-bdfc-7cd893fda9b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkgb5Y94lhYq"
      },
      "outputs": [],
      "source": [
        "embeddings_dataset = comments_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw3qKPtflhYq"
      },
      "outputs": [],
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x4f3huDlhYq",
        "outputId": "bd79e4bc-a63a-4feb-e9da-33f2e5ab1e52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "question = \"How can I load a dataset offline?\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wiWf4SUGlhYr"
      },
      "outputs": [],
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_c6m6UIDlhYr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TKe4DfS1lhYs",
        "outputId": "18746e6d-66b8-4e07-ace6-3bcd2344eb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\r\n",
            "\r\n",
            "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
            "SCORE: 25.505016326904297\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\r\n",
            "You can now use them offline\r\n",
            "```python\r\n",
            "datasets = load_dataset('text', data_files=data_files)\r\n",
            "```\r\n",
            "\r\n",
            "We'll do a new release soon\n",
            "SCORE: 24.555538177490234\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\r\n",
            "\r\n",
            "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \r\n",
            "\r\n",
            "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\r\n",
            "\r\n",
            "----------\r\n",
            "\r\n",
            "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\r\n",
            "\r\n",
            "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\r\n",
            "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\r\n",
            "```python\r\n",
            "load_dataset(\"./my_dataset\")\r\n",
            "```\r\n",
            "and the dataset script will generate your dataset once and for all.\r\n",
            "\r\n",
            "----------\r\n",
            "\r\n",
            "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\r\n",
            "cf #1724 \n",
            "SCORE: 24.14898681640625\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
            "> \n",
            "> 1. (online machine)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> import datasets\n",
            "> \n",
            "> data = datasets.load_dataset(...)\n",
            "> \n",
            "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> 2. copy the dir from online to the offline machine\n",
            "> \n",
            "> 3. (offline machine)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> import datasets\n",
            "> \n",
            "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> \n",
            "> \n",
            "> HTH.\n",
            "\n",
            "\n",
            "SCORE: 22.89400291442871\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\r\n",
            "1. (online machine)\r\n",
            "```\r\n",
            "import datasets\r\n",
            "data = datasets.load_dataset(...)\r\n",
            "data.save_to_disk(/YOUR/DATASET/DIR)\r\n",
            "```\r\n",
            "2. copy the dir from online to the offline machine\r\n",
            "3. (offline machine)\r\n",
            "```\r\n",
            "import datasets\r\n",
            "data = datasets.load_from_disk(/SAVED/DATA/DIR)\r\n",
            "```\r\n",
            "\r\n",
            "HTH.\n",
            "SCORE: 22.40665626525879\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"COMMENT: {row.comments}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"URL: {row.html_url}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create your own query and see whether you can find an answer in the retrieved documents. You might have to increase the k parameter in Dataset.get_nearest_examples() to broaden the search."
      ],
      "metadata": {
        "id": "9B9n-w9TwhoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How to protect master branch?\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ],
      "metadata": {
        "id": "YYJv7IhEvwnN",
        "outputId": "2a226c8e-6d15-4d2d-b4be-5a728deaf0c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")"
      ],
      "metadata": {
        "id": "CyNTcqmdv4H3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ],
      "metadata": {
        "id": "akOljZOcv72J"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"COMMENT: {row.comments}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"URL: {row.html_url}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ],
      "metadata": {
        "id": "z4y1WB2jwFEl",
        "outputId": "eac6b9c5-04ae-4585-9e2b-24a713022a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMENT: We can be pretty flexible and not impose any constraints for transforms.\r\n",
            "\r\n",
            "Moreover, this library is designed to support datasets bigger than memory. The datasets are loaded from the disk via memory mapping, without filling up RAM. Even processing functions like `map` work in a batched fashion to not fill up your RAM. So this shouldn't be an issue\n",
            "SCORE: 55.97919845581055\n",
            "TITLE: Transformer Class on dataset\n",
            "URL: https://github.com/huggingface/datasets/issues/2596\n",
            "==================================================\n",
            "\n",
            "COMMENT: This should be accessible now as a feature in dataset.info.features (and even have the mapping methods).\n",
            "SCORE: 54.67937469482422\n",
            "TITLE: [Feature] Keep the list of labels of a dataset as metadata\n",
            "URL: https://github.com/huggingface/datasets/issues/4\n",
            "==================================================\n",
            "\n",
            "COMMENT: Update on this: I'm computing the checksums of the data files. It will be available soon\n",
            "SCORE: 52.076271057128906\n",
            "TITLE: Add C4\n",
            "URL: https://github.com/huggingface/datasets/issues/2511\n",
            "==================================================\n",
            "\n",
            "COMMENT: @lhoestq now the 2 are implemented.\r\n",
            "\r\n",
            "Please note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).\n",
            "SCORE: 29.478790283203125\n",
            "TITLE: Protect master branch\n",
            "URL: https://github.com/huggingface/datasets/issues/2945\n",
            "==================================================\n",
            "\n",
            "COMMENT: @lhoestq now the 2 are implemented.\r\n",
            "\r\n",
            "Please note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).\n",
            "SCORE: 29.478790283203125\n",
            "TITLE: Protect master branch\n",
            "URL: https://github.com/huggingface/datasets/issues/2945\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Semantic search with FAISS (PyTorch)",
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}